{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3f5c72",
   "metadata": {},
   "source": [
    "# Unique pathogen target between human and human-virus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bf1a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of concatenated sequences: 1854\n",
      "Unique human protein sequences: 1\n",
      "Unique viral protein sequences: 1\n",
      "Total length of human protein string: 581\n",
      "Total length of viral protein string: 1273\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to read and preprocess FASTA sequences\n",
    "def read_fasta(file_path, min_length=30):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file, extracts unique sequences, filters out ambiguous residues,\n",
    "    applies length filtering, and stores sequences along with their headers.\n",
    "    \n",
    "    :param file_path: Path to the FASTA file.\n",
    "    :param min_length: Minimum allowed sequence length.\n",
    "    :return: Dictionary {sequence: [headers]}.\n",
    "    \"\"\"\n",
    "    unique_sequences = defaultdict(list)  # Dictionary to store sequences and their headers\n",
    "\n",
    "    # Open file normally or as a gzip file\n",
    "    if file_path.endswith(\".gz\"):\n",
    "        handle = gzip.open(file_path, \"rt\")  # Open compressed file in text mode\n",
    "    else:\n",
    "        handle = open(file_path, \"r\")  # Open normal FASTA file\n",
    "\n",
    "    with handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            seq = str(record.seq).strip().upper()  # Convert to uppercase, remove spaces/newlines\n",
    "            \n",
    "            # Apply length filtering and remove sequences with ambiguous residues\n",
    "            if len(seq) >= min_length and not any(x in seq for x in [\"X\", \"B\", \"Z\", \"*\", \"U\"]):\n",
    "                unique_sequences[seq].append(record.id)  # Store sequence with its header\n",
    "\n",
    "    return unique_sequences  # Returns {sequence: [headers]}\n",
    "\n",
    "# Load human and viral protein sequences\n",
    "human_proteins = read_fasta(r\"DATASET\\\\Human_prion .fasta\")\n",
    "viral_proteins = read_fasta(r\"DATASET\\\\Virus_prion.fasta\")\n",
    "\n",
    "# Convert all human and viral sequences into a single concatenated string\n",
    "human_protein_string = \"\".join(human_proteins.keys())\n",
    "viral_protein_string = \"\".join(viral_proteins.keys())\n",
    "\n",
    "# Compute total length of concatenated sequences\n",
    "total_length = len(human_protein_string) + len(viral_protein_string)\n",
    "\n",
    "# Print summary of preprocessing\n",
    "print(f\"Total length of concatenated sequences: {total_length}\")\n",
    "print(f\"Unique human protein sequences: {len(human_proteins)}\")\n",
    "print(f\"Unique viral protein sequences: {len(viral_proteins)}\")\n",
    "print(f\"Total length of human protein string: {len(human_protein_string)}\")\n",
    "print(f\"Total length of viral protein string: {len(viral_protein_string)}\")\n",
    "\n",
    "# Function to return concatenated sequences for further processing (e.g., sequence alignment)\n",
    "def get_protein_strings():\n",
    "    \"\"\"\n",
    "    Returns the concatenated protein sequences for further processing.\n",
    "    \n",
    "    :return: (human_protein_string, viral_protein_string)\n",
    "    \"\"\"\n",
    "    return human_protein_string, viral_protein_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b125a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 100 characters of human protein sequence:\n",
      "MVAEVCSMPAASAVKKPFDLRSKMGKWCHHRFPCCRGSGKSNMGTSGDHDDSFMKTLRSKMGKCCHHCFPCCRGSGTSNVGTSGDHDNSFMKTLRSKMGK\n",
      "\n",
      "Next 100 characters of viral protein sequence:\n",
      "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNI\n"
     ]
    }
   ],
   "source": [
    "# Print the first 100 characters of human_protein_string\n",
    "print(\"\\nFirst 100 characters of human protein sequence:\")\n",
    "print(human_protein_string[:100])  \n",
    "\n",
    "# Print the next 100 characters of viral_protein_string\n",
    "print(\"\\nNext 100 characters of viral protein sequence:\")\n",
    "print(viral_protein_string[:100])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3520364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique data points (rows) after filtering: 1854\n",
      "Total unique human proteins: 581\n",
      "Total unique viral proteins: 1273\n",
      "\n",
      "Hirschberg Algorithm Alignment:\n",
      "A1: M-VAEVC----SM-----------PAA---SA---VKKP---FD---LRSKMGK---------WCH--H--------RF--PCCR---G----SG-KSNM--GTS-GDHDDSFMKTLRSKMGKCCHH---C-FPCCR----G--------SGT-SN--VGTSGDHDN----S--FMKTLRSKMG--KWCCHC-FPCCRGSGKSNVG-TWGDYD-DSA--FM--EPRYHVR------RED-LDKLHRAAW--------W--GK----V----PRKDLIVMLRD-T--DMNKR--DKQKRTALHLASAN---GN---SEV-VQLLLDR-RCQ----L-------N------VLD-NKKRTALIKA---VQCQEDE-----C--VLM--L--LEHG---ADGN-IQ-DEY-----GNT---ALH-YA---------IY-NEDKLMAKA------L--L-----LYGA--DI--E-----SK--NKC-GLT---PLLL-G------VHEQKQQVVK--F-LI---------KK-----KA---N-----L---------NA--LDRY--GR----T--A------L-ILAVC-C--GSAS-IV---NLL-----LEQ--N---V------D-------V-S--S---QD----L--------S-------G-------QT-------ARE--------Y-----A---V--S--SHHHV----IC---ELL--SDYKEKQMLK--IS--S-ENSNPE-QD------LK--LTSE--E------ES--QRL---KVSE-------N-SQ--PE--KMSQEPEI-----NK----D---------C--DREVEEEIKKHGSNPVG-LPENLTNG--A---SA---GN---GDD-G----L-IP---QRKSRKP----------ENQ-----QFP-------D----TENEEYHS-DE--QND----TQ-KQLSEEQNTG------I-SQ-D--E----I--L-TNK-QK-QIEV------A-E----------K--EMN---SK----------L-SL--S--HKKE----------E-----------DLL----REN---SML-------RE--E--IAMLR----------------------L--ELD---ET--KHQ-NQ------LRE----N-------K-I--L-E------E--I---ESVK-E---K------L-----LKAIQ-----LN----------------------E---EA-LT--KTS-I\n",
      "A2: MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_fasta(file_path):\n",
    "    \"\"\"\n",
    "    Reads sequences from a FASTA file and returns a list of sequences.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        seq = \"\"\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if seq:\n",
    "                    sequences.append(seq)\n",
    "                seq = \"\"  # Reset for new sequence\n",
    "            else:\n",
    "                seq += line.strip()\n",
    "        if seq:  # Append the last sequence\n",
    "            sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "def nw_last_row(seq1, seq2, gap_penalty=-2, match_score=1, mismatch_score=-1):\n",
    "    \"\"\"\n",
    "    Computes only the last row of the Needleman-Wunsch DP table to save space.\n",
    "    \"\"\"\n",
    "    prev_row = np.zeros(len(seq2) + 1, dtype=int)\n",
    "    \n",
    "    for j in range(len(seq2) + 1):\n",
    "        prev_row[j] = j * gap_penalty\n",
    "    \n",
    "    for i in range(1, len(seq1) + 1):\n",
    "        curr_row = np.zeros(len(seq2) + 1, dtype=int)\n",
    "        curr_row[0] = i * gap_penalty\n",
    "        \n",
    "        for j in range(1, len(seq2) + 1):\n",
    "            match = prev_row[j - 1] + (match_score if seq1[i - 1] == seq2[j - 1] else mismatch_score)\n",
    "            delete = prev_row[j] + gap_penalty\n",
    "            insert = curr_row[j - 1] + gap_penalty\n",
    "            curr_row[j] = max(match, delete, insert)\n",
    "        \n",
    "        prev_row = curr_row  # Move to the next row\n",
    "    \n",
    "    return prev_row\n",
    "\n",
    "def needleman_wunsch(seq1, seq2, gap_penalty=-2, match_score=1, mismatch_score=-1):\n",
    "    \"\"\"\n",
    "    Needleman-Wunsch algorithm for base case alignment when sequences are very short.\n",
    "    \"\"\"\n",
    "    n, m = len(seq1), len(seq2)\n",
    "    dp = np.zeros((n + 1, m + 1), dtype=int)\n",
    "    \n",
    "    # Initialize the DP table\n",
    "    for i in range(n + 1):\n",
    "        dp[i][0] = i * gap_penalty\n",
    "    for j in range(m + 1):\n",
    "        dp[0][j] = j * gap_penalty\n",
    "    \n",
    "    # Fill the DP table\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            match = dp[i - 1][j - 1] + (match_score if seq1[i - 1] == seq2[j - 1] else mismatch_score)\n",
    "            delete = dp[i - 1][j] + gap_penalty\n",
    "            insert = dp[i][j - 1] + gap_penalty\n",
    "            dp[i][j] = max(match, delete, insert)\n",
    "    \n",
    "    # Traceback to get the aligned sequences\n",
    "    align1, align2 = \"\", \"\"\n",
    "    i, j = n, m\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and dp[i][j] == dp[i - 1][j - 1] + (match_score if seq1[i - 1] == seq2[j - 1] else mismatch_score):\n",
    "            align1 = seq1[i - 1] + align1\n",
    "            align2 = seq2[j - 1] + align2\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dp[i][j] == dp[i - 1][j] + gap_penalty:\n",
    "            align1 = seq1[i - 1] + align1\n",
    "            align2 = \"-\" + align2\n",
    "            i -= 1\n",
    "        else:\n",
    "            align1 = \"-\" + align1\n",
    "            align2 = seq2[j - 1] + align2\n",
    "            j -= 1\n",
    "    \n",
    "    return align1, align2\n",
    "\n",
    "def hirschberg(seq1, seq2, gap_penalty=-2, match_score=1, mismatch_score=-1):\n",
    "    \"\"\"\n",
    "    Hirschberg's Algorithm for global sequence alignment (O(n) space complexity).\n",
    "    \"\"\"\n",
    "    if len(seq1) == 0:\n",
    "        return \"-\" * len(seq2), seq2\n",
    "    if len(seq2) == 0:\n",
    "        return seq1, \"-\" * len(seq1)\n",
    "    if len(seq1) == 1 or len(seq2) == 1:\n",
    "        return needleman_wunsch(seq1, seq2, gap_penalty, match_score, mismatch_score)\n",
    "    \n",
    "    mid = len(seq1) // 2\n",
    "    left_score = nw_last_row(seq1[:mid], seq2, gap_penalty, match_score, mismatch_score)\n",
    "    right_score = nw_last_row(seq1[mid:][::-1], seq2[::-1], gap_penalty, match_score, mismatch_score)[::-1]\n",
    "    \n",
    "    # Fix: Handle tie cases explicitly\n",
    "    split_scores = left_score + right_score\n",
    "    split_idx = np.where(split_scores == np.max(split_scores))[0][0]  # Select the first max value\n",
    "\n",
    "    # Recursively align both halves\n",
    "    left_alignment = hirschberg(seq1[:mid], seq2[:split_idx], gap_penalty, match_score, mismatch_score)\n",
    "    right_alignment = hirschberg(seq1[mid:], seq2[split_idx:], gap_penalty, match_score, mismatch_score)\n",
    "    \n",
    "    return left_alignment[0] + right_alignment[0], left_alignment[1] + right_alignment[1]\n",
    "\n",
    "# Compute total unique sequences\n",
    "total_unique_rows = len(human_protein_string) + len(viral_protein_string)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Total unique data points (rows) after filtering: {total_unique_rows}\")\n",
    "print(f\"Total unique human proteins: {len(human_protein_string)}\")\n",
    "print(f\"Total unique viral proteins: {len(viral_protein_string)}\")\n",
    "\n",
    "# Perform Hirschberg alignment on full protein sequences\n",
    "align1, align2 = hirschberg(human_protein_string, viral_protein_string)\n",
    "print(\"\\nHirschberg Algorithm Alignment:\")\n",
    "print(\"A1:\", align1)\n",
    "print(\"A2:\", align2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db5b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Viral Sequence (Safe for Drug Targeting):\n",
      "FLPLVQCVNLTTRTQLYTNTRGDKVSSVFLPFFSNVTAIVSGTNGTKDNFNDVYFAEIRFIKVEDPFLVYYHKNNKEFRFEYVQPNFVHRQGALIGINITTTPGDSSSGTAAAYYGYLQGITALVEKYQTRVNITNCPFGEVFATRFASWDYSFSTFKYGTKNDNVYVGRQIAPGKINLPDDFTGCVWGGNYNYYRFRKSNERSTIYQAGPCECYFYFQPTNGLSEHAPATVCGPSTNLVKCVFNFNGTGTGVLTESKFQFDIADTDVRDPQTEPSFVPGTNQVAVDVCTEPVAIHAQLTPTWRYTGNVFRAGCIGAEHVNNYECDIPIAGICASYQTNSPRRVASQSIIATMSLGENSAYNNTNFTVTTPVMYGDTLGSFCTQRAAVQDKNTQFAQIYIKDFGGFFILPSEDLLFVTLAAGFIKQYGDLGVMIQYTLLAITSFAGAAQFAMGIGVTQNVLYKLIANSAIGKIQSLSSQVNQALNVSSVLNDLLKVAEVQDRILLTQQLIRAIRASANLAATMSLGQRVDFCGKGYHMPQAPFLHVTYVPAQKNFTTAPAICHAHFPVFVTHWFVTQFYPQNTFVSGNCDVVIGIVNNTVYDPQPSFKLDKTSPDVDISGIASVVNIQEDRNVAKNLNSLDLQYQYIWPWYIWGFIAGMVTIMCMTSCCSCLKGCCSCGSCCKFDDDSVGVY\n"
     ]
    }
   ],
   "source": [
    "def extract_unique_non_conserved_regions(aligned_human, aligned_virus):\n",
    "    \"\"\"\n",
    "    Extracts unique viral regions from the aligned sequences by:\n",
    "    1. Identifying positions where the human protein has gaps (\"-\").\n",
    "    2. Removing viral residues that match human residues (conserved regions).\n",
    "    \n",
    "    :param aligned_human: Aligned human protein sequence (A1)\n",
    "    :param aligned_virus: Aligned viral protein sequence (A2)\n",
    "    :return: Unique viral sequence without human similarity\n",
    "    \"\"\"\n",
    "    unique_viral_seq = \"\"\n",
    "\n",
    "    for i in range(len(aligned_human)):\n",
    "        # Keep viral sequence only if:\n",
    "        # - The human sequence has a gap (viral-specific region)\n",
    "        # - The viral residue is NOT the same as the human residue (not conserved)\n",
    "        if aligned_human[i] == \"-\" and aligned_virus[i] != \"-\":\n",
    "            unique_viral_seq += aligned_virus[i]\n",
    "\n",
    "    return unique_viral_seq\n",
    "\n",
    "# Extract unique viral sequence\n",
    "unique_viral_sequence = extract_unique_non_conserved_regions(align1, align2)\n",
    "\n",
    "print(\"\\nUnique Viral Sequence (Safe for Drug Targeting):\")\n",
    "print(unique_viral_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046a06aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 21.45%\n"
     ]
    }
   ],
   "source": [
    "def similarity_score(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Computes the similarity score between two aligned sequences and returns a percentage.\n",
    "    \n",
    "    Parameters:\n",
    "    - seq1 (str): First aligned sequence (e.g., human protein).\n",
    "    - seq2 (str): Second aligned sequence (e.0.g., viral protein).\n",
    "    \n",
    "    Returns:\n",
    "    - float: Similarity score out of 100.\n",
    "    \"\"\"\n",
    "    if len(seq1) != len(seq2):\n",
    "        raise ValueError(\"Sequences must be aligned and of the same length.\")\n",
    "\n",
    "    match_score = 1  # Points for a match\n",
    "    total_length = len(seq1)  # Length of aligned sequences\n",
    "    match_count = sum(1 for a, b in zip(seq1, seq2) if a == b and a != \"-\")\n",
    "\n",
    "    similarity_percentage = (match_count / total_length) * 100\n",
    "    return round(similarity_percentage, 2)\n",
    "\n",
    "# Example Usage\n",
    "seq1 = align1\n",
    "seq2 = align2\n",
    "score = similarity_score(seq1, seq2)\n",
    "print(f\"Similarity Score: {score}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba65df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from Bio import PDB\n",
    "\n",
    "def ramachandran_plot(pdb_file):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"protein\", pdb_file)\n",
    "    phi_psi_angles = []\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            polypeptides = PDB.PPBuilder().build_peptides(chain)\n",
    "            for poly_index, poly in enumerate(polypeptides):\n",
    "                phi_psi_angles.extend(poly.get_phi_psi_list())\n",
    "\n",
    "    # Extract valid angles\n",
    "    phi_angles = [angle[0] for angle in phi_psi_angles if angle[0] is not None]\n",
    "    psi_angles = [angle[1] for angle in phi_psi_angles if angle[1] is not None]\n",
    "\n",
    "    # Plot Ramachandran Plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.xlim(-180, 180)\n",
    "    plt.ylim(-180, 180)\n",
    "    plt.xlabel(\"Phi (ϕ) angles\")\n",
    "    plt.ylabel(\"Psi (ψ) angles\")\n",
    "    plt.title(\"Ramachandran Plot\")\n",
    "    plt.scatter(phi_angles, psi_angles, s=10, color=\"blue\", alpha=0.5)\n",
    "    plt.axhline(0, color=\"black\", linewidth=0.5)\n",
    "    plt.axvline(0, color=\"black\", linewidth=0.5)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# ramachandran_plot(\"C:\\\\Users\\\\vikas\\\\Downloads\\\\model1 (1).pdb\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2932081",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [7.5, 7.4, 7.3, 7.1, 7.0, 7.0, 7.0, 7.0]  # affinity energy \n",
    "weights = [0.0, 1.27, 2.373, 4.301, 4.072, 3.689, 4.108, 4.214]  # rmsd/lb\n",
    "max_weight = 10.0  # Seting a cutoff for total rmsd/lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b86a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack(values, weights, max_weight):\n",
    "    n = len(values)\n",
    "    W = int(max_weight * 100)  # scale to integers\n",
    "    wt = [int(w * 100) for w in weights]\n",
    "    \n",
    "    dp = [[0] * (W + 1) for _ in range(n + 1)]\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        for w in range(W + 1):\n",
    "            if wt[i - 1] <= w:\n",
    "                dp[i][w] = max(dp[i - 1][w], dp[i - 1][w - wt[i - 1]] + values[i - 1])\n",
    "            else:\n",
    "                dp[i][w] = dp[i - 1][w]\n",
    "    \n",
    "    # Backtrack to find selected ligands\n",
    "    res = dp[n][W]\n",
    "    w = W\n",
    "    selected = []\n",
    "    \n",
    "    for i in range(n, 0, -1):\n",
    "        if res <= 0:\n",
    "            break\n",
    "        if res == dp[i - 1][w]:\n",
    "            continue\n",
    "        else:\n",
    "            selected.append(i - 1)\n",
    "            res -= values[i - 1]\n",
    "            w -= wt[i - 1]\n",
    "    \n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "952cfe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Ligands:\n",
      "Rank 1: Energy = -7.5 kcal/mol, rmsd/lb = 0.0 Å\n",
      "Rank 2: Energy = -7.4 kcal/mol, rmsd/lb = 1.27 Å\n",
      "Rank 3: Energy = -7.3 kcal/mol, rmsd/lb = 2.373 Å\n",
      "Rank 4: Energy = -7.1 kcal/mol, rmsd/lb = 4.301 Å\n"
     ]
    }
   ],
   "source": [
    "selected = knapsack(values, weights, max_weight)\n",
    "\n",
    "print(\"Selected Ligands:\")\n",
    "for i in selected[::-1]:  # Reverse for original order\n",
    "    print(f\"Rank {i+1}: Energy = {-values[i]} kcal/mol, rmsd/lb = {weights[i]} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac0bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Pocket ID: 1 with Vol_ms = 258435.017\n",
      "Center of Pocket 1: (162.197, 150.962, 157.402)\n",
      "Coordinates saved to best_pocket_coordinates.txt\n"
     ]
    }
   ],
   "source": [
    "def parse_pocinfo(file_path):\n",
    "    \"\"\"\n",
    "    Parse the .pocInfo file and return the pocket with the highest Vol_ms.\n",
    "    Returns a dictionary with keys 'id' and 'volume'.\n",
    "    \n",
    "    Expected file format example (tab-separated):\n",
    "    \n",
    "    POC:\tMolecule\tID\tN_mth\tArea_sa\tArea_ms\tVol_sa\tVol_ms\tLenth\tcnr\n",
    "    POC:\tj_67f7907b2f292\t1\t83\t46455.182\t51202.342\t191294.408\t258435.017\t28589.138\t10852\n",
    "    ...\n",
    "    \"\"\"\n",
    "    best_pocket = None\n",
    "    max_volume = -1\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip header line if present\n",
    "            if line.startswith(\"POC:\") and \"Molecule\" in line:\n",
    "                continue\n",
    "            \n",
    "            if line.startswith(\"POC:\"):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 7:\n",
    "                    try:\n",
    "                        # parts[2] is the pocket ID (as string) and parts[6] is Vol_sa, parts[7] is Vol_ms\n",
    "                        pocket_id = int(parts[2])\n",
    "                        vol_ms = float(parts[7])  # Use Vol_ms (index 7)\n",
    "                        if vol_ms > max_volume:\n",
    "                            max_volume = vol_ms\n",
    "                            best_pocket = {'id': pocket_id, 'volume': vol_ms}\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    if best_pocket:\n",
    "        print(f\"Best Pocket ID: {best_pocket['id']} with Vol_ms = {best_pocket['volume']}\")\n",
    "    else:\n",
    "        print(\"No valid pocket data found.\")\n",
    "    \n",
    "    return best_pocket\n",
    "\n",
    "\n",
    "def extract_poc_coordinates(file_path, target_pocket_id):\n",
    "    \"\"\"\n",
    "    Extract the coordinates (x, y, z) from the .poc file for the target pocket.\n",
    "    \n",
    "    For this sample .poc file, we assume that ATOM records belong to a single pocket \n",
    "    (target_pocket_id). If the file contains records for multiple pockets, additional \n",
    "    logic is needed to separate them.\n",
    "    \n",
    "    Expected .poc file sample (each ATOM record is space-delimited):\n",
    "    ATOM      1  N   GLN A   6     164.246 117.137 190.085  1.00  0.00   1  POC\n",
    "    ...\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Process only lines that start with \"ATOM\"\n",
    "            if line.startswith(\"ATOM\"):\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 9:\n",
    "                    try:\n",
    "                        # According to the sample:\n",
    "                        # parts[6] = x, parts[7] = y, parts[8] = z\n",
    "                        x, y, z = map(float, (parts[6], parts[7], parts[8]))\n",
    "                        coords.append((x, y, z))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    if not coords:\n",
    "        print(f\"No coordinates found for Pocket {target_pocket_id}.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the centroid (average coordinates) of the pocket\n",
    "    x_avg = sum(coord[0] for coord in coords) / len(coords)\n",
    "    y_avg = sum(coord[1] for coord in coords) / len(coords)\n",
    "    z_avg = sum(coord[2] for coord in coords) / len(coords)\n",
    "    \n",
    "    print(f\"Center of Pocket {target_pocket_id}: ({x_avg:.3f}, {y_avg:.3f}, {z_avg:.3f})\")\n",
    "    return (x_avg, y_avg, z_avg)\n",
    "\n",
    "\n",
    "# === Replace these paths with your actual file locations ===\n",
    "pocinfo_path = \"j_67f7907b2f292.pocInfo\"\n",
    "poc_path = \"j_67f7907b2f292.poc\"\n",
    "\n",
    "# === Run the pocket selection and coordinate extraction ===\n",
    "best_pocket = parse_pocinfo(pocinfo_path)\n",
    "\n",
    "if best_pocket is not None:\n",
    "    # For this example, we assume all ATOM records in the .poc file belong to pocket ID 1.\n",
    "    coordinates = extract_poc_coordinates(poc_path, best_pocket['id'])\n",
    "\n",
    "    if coordinates is not None:\n",
    "        with open(\"best_pocket_coordinates.txt\", \"w\") as f:\n",
    "            f.write(f\"{coordinates[0]:.3f}, {coordinates[1]:.3f}, {coordinates[2]:.3f}\\n\")\n",
    "        print(\"Coordinates saved to best_pocket_coordinates.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
